{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: carla in /home/azzy13/anaconda3/envs/carla/lib/python3.8/site-packages (0.9.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python in /home/azzy13/anaconda3/envs/carla/lib/python3.8/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/azzy13/anaconda3/envs/carla/lib/python3.8/site-packages (from opencv-python) (1.18.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pygame in /home/azzy13/anaconda3/envs/carla/lib/python3.8/site-packages (2.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install carla \n",
    "%pip install opencv-python\n",
    "%pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.0 (SDL 2.28.4, Python 3.8.19)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Import the CARLA Python API library \n",
    "import carla\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import queue\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pygame\n",
    "from PIL import Image\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the client and get the world object\n",
    "client = carla.Client('localhost', 2000) \n",
    "client.set_timeout(200.0)\n",
    "world  = client.reload_world()\n",
    "bp_lib = world.get_blueprint_library()\n",
    "spawn_points = world.get_map().get_spawn_points() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small town with rain and river\n",
    "#client.load_world('Town01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change weather to rainy\n",
    "# weather = carla.WeatherParameters(\n",
    "#     cloudiness=30.0,\n",
    "#     precipitation=70.0,\n",
    "#     sun_altitude_angle=70.0)\n",
    "\n",
    "# world.set_weather(weather)\n",
    "\n",
    "# print(world.get_weather())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for bp in world.get_blueprint_library().filter('vehicle'):\n",
    "    #print(bp.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the simulator in synchronous mode\n",
    "settings = world.get_settings()\n",
    "settings.synchronous_mode = True # Enables synchronous mode\n",
    "settings.fixed_delta_seconds = 0.01\n",
    "world.apply_settings(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the blueprint for the vehicle you want\n",
    "vehicle_bp = bp_lib.find('vehicle.nissan.patrol_2021') \n",
    "\n",
    "# Try spawning the vehicle at a randomly chosen spawn point\n",
    "vehicle = world.try_spawn_actor(vehicle_bp, random.choice(spawn_points))\n",
    "spectator = world.get_spectator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the spectator behind the vehicle \n",
    "#spectator = world.get_spectator() \n",
    "#transform = carla.Transform(vehicle.get_transform().transform(carla.Location(x=-4,z=2.5)),vehicle.get_transform().rotation) \n",
    "#spectator.set_transform(transform) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location(x=-7.185984, y=113.806297, z=15.188178) Rotation(pitch=-15.657955, yaw=167.534378, roll=0.000065)\n"
     ]
    }
   ],
   "source": [
    "# Get the spectator actor\n",
    "spectator = world.get_spectator()\n",
    "\n",
    "# Get the position and orientation of the spectator\n",
    "spectator_transform = spectator.get_transform()\n",
    "spectator_position = spectator_transform.location\n",
    "spectator_rotation = spectator_transform.rotation\n",
    "print(spectator_position,spectator_rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spawn vehicles\n",
    "for i in range(50): \n",
    "    vehicle_bp = random.choice(bp_lib.filter('vehicle')) \n",
    "    npc = world.try_spawn_actor(vehicle_bp, random.choice(spawn_points)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the all vehicles in motion using the Traffic Manager\n",
    "#for v in world.get_actors().filter('*vehicle*'): \n",
    "#    v.set_autopilot(True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spawn an RGB cammera with an offset from the vehicle center\n",
    "#camera_bp = bp_lib.find('sensor.camera.rgb') \n",
    "#camera_init_trans = carla.Transform(vehicle.get_transform().transform(carla.Location(x=-4,z=2.5)),vehicle.get_transform().rotation) \n",
    "#camera = world.spawn_actor(camera_bp, camera_init_trans, attach_to=vehicle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntraffic_light = world.get_actors().filter('traffic.traffic_light')[3]\\nprint(traffic_light)  \\np0 = traffic_light.get_transform()\\ncamera_traffic = bp_lib.find('sensor.camera.rgb')\\n# Modify the attributes of the blueprint to set image resolution and field of view.\\n#camera_traffic.set_attribute('image_size_x', '1920')\\n#camera_traffic.set_attribute('image_size_y', '1080')\\ncamera_traffic.set_attribute('fov', '110')\\n# Set the time in seconds between sensor captures\\ncamera_traffic.set_attribute('sensor_tick', '2.0')\\ncamera = world.spawn_actor(camera_traffic, p0, attach_to=traffic_light)\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# camera on traffic lights\n",
    "'''\n",
    "traffic_light = world.get_actors().filter('traffic.traffic_light')[3]\n",
    "print(traffic_light)  \n",
    "p0 = traffic_light.get_transform()\n",
    "camera_traffic = bp_lib.find('sensor.camera.rgb')\n",
    "# Modify the attributes of the blueprint to set image resolution and field of view.\n",
    "#camera_traffic.set_attribute('image_size_x', '1920')\n",
    "#camera_traffic.set_attribute('image_size_y', '1080')\n",
    "camera_traffic.set_attribute('fov', '110')\n",
    "# Set the time in seconds between sensor captures\n",
    "camera_traffic.set_attribute('sensor_tick', '2.0')\n",
    "camera = world.spawn_actor(camera_traffic, p0, attach_to=traffic_light)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location(x=14.000000, y=29.000000, z=6.000000) Rotation(pitch=0.000000, yaw=1.570796, roll=0.000000)\n"
     ]
    }
   ],
   "source": [
    "#camera on spectator (drone)\n",
    "#drone = world.get_spectator()\n",
    "#transform = drone.get_transform()\n",
    "#location = transform.location\n",
    "#rotation = transform.rotation\n",
    "#Location(x=-57.660717, y=-71.480591, z=9.112691) Rotation(pitch=-8.841427, yaw=42.202526, roll=0.000119)\n",
    "#location = carla.Location(x=14.0, y=29.0, z=6.0)\n",
    "#rotation = carla.Rotation(pitch=0.0, yaw=math.pi/2, roll=0.0)\n",
    "location = carla.Location(x=-57.6,y=-71.5,z=9.1)\n",
    "rotation = carla.Rotation(pitch=-8.85, yaw=42.2, roll=0.0)\n",
    "transform = carla.Transform(location, rotation)\n",
    "# Set the spectator with an empty transform\n",
    "#drone.set_transform(carla.Transform())\n",
    "\n",
    "camera_drone = bp_lib.find('sensor.camera.rgb')\n",
    "camera_drone.set_attribute('image_size_x', '1920')\n",
    "camera_drone.set_attribute('image_size_y', '1080')\n",
    "camera_drone.set_attribute('sensor_tick', '1.0')\n",
    "camera_drone.set_attribute('fov', '110')\n",
    "camera = world.spawn_actor(camera_drone, transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instance segmentation camera on drone\n",
    "# drone = world.get_spectator()\n",
    "# transform = drone.get_transform()\n",
    "# location = transform.location\n",
    "# rotation = transform.rotation\n",
    "# # Set the spectator with an empty transform\n",
    "# drone.set_transform(carla.Transform())\n",
    "\n",
    "seg_camera = bp_lib.find('sensor.camera.instance_segmentation')\n",
    "seg_camera.set_attribute('image_size_x', '1920')\n",
    "seg_camera.set_attribute('image_size_y', '1080')\n",
    "seg_camera.set_attribute('sensor_tick', '1.0')\n",
    "seg_camera.set_attribute('fov', '110')\n",
    "segmentation_camera = world.spawn_actor(seg_camera, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #semantic lidar on drone\n",
    "# drone = world.get_spectator()\n",
    "# transform = drone.get_transform()\n",
    "# location = transform.location\n",
    "# rotation = transform.rotation\n",
    "# # Set the spectator with an empty transform\n",
    "# drone.set_transform(carla.Transform())\n",
    "\n",
    "sem_lidar = bp_lib.find('sensor.lidar.ray_cast_semantic')\n",
    "sem_lidar.set_attribute('sensor_tick', '1.0')\n",
    "lidar = world.spawn_actor(sem_lidar, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print lidar data\n",
    "def semantic_lidar_data(point_cloud_data):\n",
    "  for detection in point_cloud_data:\n",
    "    print(detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the camera saving data to disk (rgb)\n",
    "#camera.listen(lambda image: image.save_to_disk('dataset/stationary_camera/rgb/%06d.png' % image.frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save segmented images\n",
    "#segmentation_camera.listen(lambda image: image.save_to_disk('dataset/stationary_camera/seg_cam/%06d.png' % image.frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Lidar info\n",
    "#lidar.listen(lambda point_cloud_data: semantic_lidar_data(point_cloud_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#camera.stop()\n",
    "#segmentation_camera.stop()\n",
    "#lidar.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Camera geometric projection -> 3D points\n",
    "def build_projection_matrix(w, h, fov, is_behind_camera=False):\n",
    "    focal = w / (2.0 * np.tan(fov * np.pi / 360.0))\n",
    "    K = np.identity(3)\n",
    "\n",
    "    if is_behind_camera:\n",
    "        K[0, 0] = K[1, 1] = -focal\n",
    "    else:\n",
    "        K[0, 0] = K[1, 1] = focal\n",
    "\n",
    "    K[0, 2] = w / 2.0\n",
    "    K[1, 2] = h / 2.0\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D to 2D\n",
    "def get_image_point(loc, K, w2c):\n",
    "        # Calculate 2D projection of 3D coordinate\n",
    "\n",
    "        # Format the input coordinate (loc is a carla.Position object)\n",
    "        point = np.array([loc.x, loc.y, loc.z, 1])\n",
    "        # transform to camera coordinates\n",
    "        point_camera = np.dot(w2c, point)\n",
    "\n",
    "        # New we must change from UE4's coordinate system to an \"standard\"\n",
    "        # (x, y ,z) -> (y, -z, x)\n",
    "        # and we remove the fourth componebonent also\n",
    "        point_camera = [point_camera[1], -point_camera[2], point_camera[0]]\n",
    "\n",
    "        # now project 3D->2D using the camera matrix\n",
    "        point_img = np.dot(K, point_camera)\n",
    "        # normalize\n",
    "        point_img[0] /= point_img[2]\n",
    "        point_img[1] /= point_img[2]\n",
    "\n",
    "        return point_img[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the world to camera matrix\n",
    "world_2_camera = np.array(camera.get_transform().get_inverse_matrix())\n",
    "\n",
    "# Get the attributes from the camera\n",
    "image_w = camera_drone.get_attribute(\"image_size_x\").as_int()\n",
    "image_h = camera_drone.get_attribute(\"image_size_y\").as_int()\n",
    "fov = camera_drone.get_attribute(\"fov\").as_float()\n",
    "\n",
    "# Calculate the camera projection matrix to project from 3D -> 2D\n",
    "K = build_projection_matrix(image_w, image_h, fov)\n",
    "K_b = build_projection_matrix(image_w, image_h, fov, is_behind_camera=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_queue = queue.Queue()\n",
    "camera.listen(image_queue.put)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "2\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "3\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "4\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "5\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "6\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "7\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "8\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "9\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "10\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "11\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "12\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "13\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "14\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "15\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "16\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "17\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "18\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "19\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "20\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "21\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "22\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "23\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "24\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n",
      "25\n",
      "Received image!\n",
      "[-1834.0, 887.0, -311.0, 1532.0]\n",
      "[-560.0, -513.0, 3368.0, 1176.0]\n",
      "[-33341.0, -4399.0, 9286.0, 1761.0]\n",
      "[885.0, 583.0, 909.0, 607.0]\n"
     ]
    }
   ],
   "source": [
    "width = 1920\n",
    "height = 1080\n",
    "\n",
    "pygame.init()\n",
    "window = pygame.display.set_mode(size=(width,height))\n",
    "font = pygame.font.SysFont(None, 24)\n",
    "video_surf = None\n",
    "\n",
    "world.tick()\n",
    "# Set the all vehicles in motion using the Traffic Manager\n",
    "for v in world.get_actors().filter('*vehicle*'): \n",
    "    v.set_autopilot(True)\n",
    "\n",
    "simulation_dataset = {}\n",
    "simulation_dataset[\"images\"] = []\n",
    "simulation_dataset[\"annotations\"] = []\n",
    "\n",
    "#render 2D bboxes\n",
    "running = True\n",
    "tick = 0\n",
    "max_tick = 12000\n",
    "while running:\n",
    "    tick = tick + 1\n",
    "    print(tick)\n",
    "    if (tick > max_tick):\n",
    "        break\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "    # Retrieve and reshape the image\n",
    "    world.tick()\n",
    "    image = None\n",
    "    try:\n",
    "        image = image_queue.get(timeout=1)\n",
    "        print(\"Received image!\")\n",
    "    except:\n",
    "        print(\"No image!\")\n",
    "        continue\n",
    "\n",
    "    img = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    img_filename = 'dataset/stationary_camera/rgb/%06d.png' % image.frame\n",
    "    img_bgr = img[:,:, :3]\n",
    "    #print(img_bg)\n",
    "    img_rgb = img_bgr[:,:, ::-1]\n",
    "    img_PIL = Image.fromarray(img_rgb)\n",
    "    img_PIL.save(img_filename)\n",
    "    simulation_dataset[\"images\"].append({\n",
    "        \"file_name\": img_filename,\n",
    "        \"height\": 1920,\n",
    "        \"width\": 1080,\n",
    "        \"id\": image.frame\n",
    "    })\n",
    "\n",
    "    # Get the camera matrix \n",
    "    world_2_camera = np.array(camera.get_transform().get_inverse_matrix())\n",
    "\n",
    "    for npc in world.get_actors().filter('*vehicle*'):\n",
    "\n",
    "        # Filter out the ego vehicle\n",
    "        if npc.id != vehicle.id:\n",
    "\n",
    "            bb = npc.bounding_box\n",
    "            dist = npc.get_transform().location.distance(transform.location)\n",
    "\n",
    "            # Filter for the vehicles within 50m\n",
    "            if dist < 70:\n",
    "\n",
    "            # Calculate the dot product between the forward vector\n",
    "            # of the vehicle and the vector between the vehicle\n",
    "            # and the other vehicle. We threshold this dot product\n",
    "            # to limit to drawing bounding boxes IN FRONT OF THE CAMERA\n",
    "                forward_vec = transform.get_forward_vector()\n",
    "                ray = npc.get_transform().location - transform.location\n",
    "\n",
    "                if forward_vec.dot(ray) > 0:\n",
    "                    p1 = get_image_point(bb.location, K, world_2_camera)\n",
    "                    verts = [v for v in bb.get_world_vertices(npc.get_transform())]\n",
    "                    x_max = -10000\n",
    "                    x_min = 10000\n",
    "                    y_max = -10000\n",
    "                    y_min = 10000\n",
    "\n",
    "                    for vert in verts:\n",
    "                        p = get_image_point(vert, K, world_2_camera)\n",
    "                        # Find the rightmost vertex\n",
    "                        if p[0] > x_max:\n",
    "                            x_max = p[0]\n",
    "                        # Find the leftmost vertex\n",
    "                        if p[0] < x_min:\n",
    "                            x_min = p[0]\n",
    "                        # Find the highest vertex\n",
    "                        if p[1] > y_max:\n",
    "                            y_max = p[1]\n",
    "                        # Find the lowest  vertex\n",
    "                        if p[1] < y_min:\n",
    "                            y_min = p[1]\n",
    "\n",
    "                    cv2.line(img, (int(x_min),int(y_min)), (int(x_max),int(y_min)), (0,0,255, 255), 1)\n",
    "                    cv2.line(img, (int(x_min),int(y_max)), (int(x_max),int(y_max)), (0,0,255, 255), 1)\n",
    "                    cv2.line(img, (int(x_min),int(y_min)), (int(x_min),int(y_max)), (0,0,255, 255), 1)\n",
    "                    cv2.line(img, (int(x_max),int(y_min)), (int(x_max),int(y_max)), (0,0,255, 255), 1)\n",
    "                    bbox = [x_min, y_min, x_max, y_max]\n",
    "                    # Draw vertices\n",
    "                    vertices = [(x_min, y_min), (x_max, y_min), (x_min, y_max), (x_max, y_max)]\n",
    "                    # for vertex in vertices:\n",
    "                    #     cv2.circle(img, vertex, 5, (0, 255, 0), -1)  # Draw a circle at each vertex\n",
    "\n",
    "                    # Put the text for each vertex\n",
    "                    # for vertex in vertices:\n",
    "                    #     cv2.putText(img, str(vertex), (vertex[0] + 10, vertex[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "                    # Convert bounding box coordinates to string\n",
    "                    bbox = [x_min, y_min, x_max, y_max]\n",
    "                    bbox_str = str(bbox)\n",
    "\n",
    "                    # Put the bounding box string on the image\n",
    "                    # cv2.putText(img, bbox_str, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "                    \n",
    "                    rounded_bbox = [round(value) for value in bbox]\n",
    "                    print(rounded_bbox)\n",
    "                    simulation_dataset[\"annotations\"].append({\n",
    "                        \"image_id\": image.frame,\n",
    "                        \"bbox\": rounded_bbox\n",
    "                    })\n",
    "\n",
    "    video_surf = pygame.image.frombuffer(img, (image.width, image.height), \"BGRA\")\n",
    "    window.blit(video_surf, (0,0))\n",
    "    pygame.display.flip()\n",
    "    #pygame.time.wait(5)\n",
    "pygame.quit()\n",
    "\n",
    "with open('simulation_data.json', 'w') as json_file:\n",
    "    json.dump(simulation_dataset, json_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box = vehicle.bounding_box\n",
    "# print(box.location)         # Location relative to the vehicle.\n",
    "# print(box.extent)\n",
    "#cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
